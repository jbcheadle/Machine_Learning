---
title: "Practical Machine Learning Course Project"
author: "John B Cheadle"
date: "January 17, 2018"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(caret)
require(ggplot2)
require(RANN)
require(rattle)
require(e1071)
require(randomForest)
```

# Executive Summary

The goal of this report is to use accelerometer data from participants engaged in a dumbbell exercise, predicting the manner in which they did the exercise (classe) from variables in the dataset using a machine learning algorithm.  Data was first preprocessed to remove columns in which most values were NAs, impertinent data columns, and columns that had near-zero variation.  The data was split up into training and validation sets; the 'test' set is actually a set of 20 observations used for a quiz in the Practical Machine Learning course at Coursera.  Two models were generated using the rpart and rf machine learning algorithms (caret package).  These models were chosen primarily due to number of predictors and size of dataset; they are suited to very large datasets with a large number of variables, where the outcome is a factor and the features are automatically selected.  Cross-validation was performed within each train() function - k-folds cross-validation (k=5) was chosen due to its lighter computational burden compared with other methods such as repeated k-folds cross validation.  the rpart model was a poor predictor of exercise class, with accuracy = 0.5051, whereas the rf model was an excellent predictor of exercise class, with accuracy = 1.  Therefore, our expected out-of-sample error rate is 0.

# Introduction

In this course project for the Machine Learning class at Coursera, we are working with accelerometer data used for human activity recognition (HAR).  Specifically, study data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants is used.  The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways:

* exactly according to the specification (Class A)
* throwing the elbows to the front (Class B)
* lifting the dumbbell only halfway (Class C)
* lowering the dumbbell only halfway (Class D)
* throwing the hips to the front (Class E)

The goal of this project is to predict the manner in which the excercise was performed based on the other variables in the dataset.  More information on the dataset can be found [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).  

# Components of the Model

## Question
We are trying to determine, as accurately as possible, **which type of exercise (above) was performed** by using other variables as predictors in our machine learning algorithm.

## Input Data
The data for this project are available at the following:

```{r input_data, echo=FALSE}
## Training Data
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))

## Testing Data
testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

```

The data for this project come from [this source](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har), as stated above.

## Features
Feature selection is perhaps the most important part (next to the question) of a sucessful prediction. Good features make your model more accurate and interpretable.  In this section I go over rationale for pre-processing which reduces total number of possible predictors, whereas feature selection is done automatically by many of the caret models (rpart, rf, gbm, etc).

### Preprocessing
There are a variety of pre-processing steps to take to whittle down potential features prior to model fitting.  First, I take a human look at the structure of the dataset and figure out which variables might be impertinent; e.g., which variables aren't really measures?  The first 7 were removed in this way: 'X' (index variable), 'user_name' (name of participant), 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp' (time information), 'new_window', 'num_window' (sequentiall-valued variables which I assume have to do with when a new exercise starts - no codebook available for this database, unfortunately). Secondly, columns which contain majority NA values are removed using a custom function. Finally, any variables with near-zero variance are removed using the nearZeroVariance() function.  

```{r preprocessing}

## Taking a look at the data
str(training)

# Determining amount of NAs
na_count <-sapply(training, function(y) sum(length(which(is.na(y)))))
head(na_count,20)

# Many columns have 19216 NA values (truncated), which is the majority of observations.
# These columns were removed from the training data set.
nas <- sapply(na_count, function(y) y==19216)
training <- training[,nas==FALSE]
sum(is.na(training))

# Seems like the first 7 variables are impertinent
training <- training[,-c(1:7)]

## Near-zero Variance
nzv <- nearZeroVar(training, saveMetrics = TRUE)
sum(nzv$nzv)

# 33 of our remaining variables have near-zero variance, and are removed
training <- training[,nzv$nzv == FALSE]
```
After preprocessing, we are left with 53 variables; 52 predictors and 1 outcome (classe).

Our test data set is very small (20 observations), and in order to test multiple models we can split our training set into a *training* and *validation* set prior to applying it to a test set.

```{r splitting}
inBuild <- createDataPartition(y = training$classe,
                               p=0.7, list=FALSE)
training <- training[inBuild,]
validation <- training[-inBuild,]

```

## Algorithms
There are too many variables to reasonably choose features using exploratory plots; therefore, I will use decision tree (rpart) and random forest (rf), both of which are suited to very large datasets with a large number of variables, where the outcome is a factor and the features are automatically selected.

Because random forest with repeated k-fold cross-validation was too computationally intensive for my computer, I instead use k-fold cross-validation with k=5.  Prior to the training, the seed is set for reproducibility.

```{r rpart}
set.seed(86)
train_control <- trainControl(method="cv", number=5)
mod_rpart <- train(classe ~ ., method="rpart",
                   trControl = train_control,
                   data=training)

fancyRpartPlot(mod_rpart$finalModel)
```
Our initial decision tree does an acceptable job splitting out group E,  but has trouble with the other groups (and does not even include D). 

To obtain the accuracy of the model, I predict with the validation data set, then calculate the confusion matrix.

```{r rpart_conf}
pred_rpart <- predict(mod_rpart,validation)
confm_rpart <- confusionMatrix(validation$classe, pred_rpart)
confm_rpart

```
The confusion matrix supports the initial plot.  The rpart model performs poorly, achieving an accuracy of 0.5051.

Next we move to the random forest model
```{r rf}
mod_rf <- train(classe ~ ., method="rf",
                   trControl = train_control,
                   data = training,
                   prox = TRUE)

plot(mod_rf)
```
A simple plot shows an accuracy peak at ~28 predictors.

```{r rf_conf}
pred_rf <- predict(mod_rf,validation)
confm_rf <- confusionMatrix(validation$classe, pred_rf)
confm_rf
```
The accuracy is shown to be 1, giving us an out-of-sample error rate of 0.

## Evaluation on Test Set
Training and testing data sets must be processed in the same way. Therefore, all processing we performed on the training set is performed on the test set:

```{r test_preprocess}
na_count <-sapply(testing, function(y) sum(length(which(is.na(y)))))

nas <- sapply(na_count, function(y) y==20) # Change the number of NAs to reflect testing set size
testing <- testing[,nas==FALSE]
sum(is.na(training))

# Seems like the first 7 variables are impertinent
testing <- testing[,-c(1:7)]

## Near-zero Variance
nzvtest <- nearZeroVar(testing, saveMetrics = TRUE)
sum(nzvtest$nzv)

# No near-zero variables so we don't include.
```

The test set has the variable 'problem_id' instead of 'classe' so we can't get a confusion matrix and therefore assess accuracy.  This test set is specifically for the quiz for the Practical Machine Learning class at Coursera.
```{r evaluate}
pred_rf_test <- predict(mod_rf,testing)
solutions <- as.data.frame(cbind(testing$problem_id,
                                 as.character(pred_rf_test)))
colnames(solutions) <- c("Problem ID","Answer")
solutions

```


